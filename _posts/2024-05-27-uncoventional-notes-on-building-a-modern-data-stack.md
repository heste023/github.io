Over the past few months, I've been working with a team to build a modern data and analytics reporting stack. When you work in consulting, one of the challenges that you face is that there are almost always other systems and tools that have been chosen for you and you have to figure out how they can work together. I've been helping a client recently, though, where we are choosing the tools, building everything from the ground up. 

As a data scientist, it's rare that you get to be involved in these decisions. Data science work really is the top-level of the hierarchy in data work, analogous to the self-fulfillment level in Maslow's Hierarchy of needs. So throughout the project, I've been reminded pretty regularly about the thousands of decisions that go into a full system build that I usually don't have to worry about. And since writing helps me clarify my thinking, I thought it would be good to make note of some of the lessons learned. 

1 - One of the more undervalued skills for data-related work is being able to build synthetic datasets that mirror the structure of your actual data. There is a causal structure for all of the data spigots within an organization. If you can simulate that data structure correctly, then you probably understand it. And if you understand it, you can use it to provide value. The simulation-based approach to understanding is becoming more common in [scientific research](https://elevanth.org/blog/2023/06/13/science-and-the-dumpster-fire/) but is rarely talked about as something that can help in doing data and analytics work. 

For this project, the business knew what data and reporting would help them do their work, but we had to discover the data models that they needed on our own. And that required us to understand the process for how the data came to be and all of the ways that the data could be generated. One of the best things that we did to get to this level of understanding was to generate fake data that looked and operated like it came from the actual application that we were wanting to report on. It also kept us moving forward while we were waiting around for requests to get approved and new tools to get provisioned. 

2 - If you're going to be doing anything with data that's sensitive (or might be perceived as sensitive!), security will be the biggest headache. Security people can be prickly, and rightfully so - moving data around the internet can be risky if not done right, so careful attention needs to be paid to piping things around in a way that's secure. So when you're picking tools, pick tools that are robust to security and make sure that you can prove it to your security team. 

Some highlights: 
- Encryption - As your little information packets are being ported around the digital world, they need to remain anonymous, even if they are picked off by a unauthorized party. Encryption is the baseline; it doesn't ensure anything, but it makes it hard to snuff up those packets and challenging for bad actors to do anything with your data if it ends up in the wrong hands. So pick tools that have strong encryption standards. For an extra layer of protection, you can move things around without using the public internet through private links, but you'll have to pay for it. 
- Passwords/multi-factor authentication - One of the easiest ways to strengthen the security posture of any system is to require multi-factor authentication for end-users when logging in to your system. In my opinion, multi-factor authentication is the biggest bang that you can get for your buck in security planning.
- Monitoring - For any system, you'll want to monitor both the system performance and for foul play. There are some good tools out there that can help automate this work, but most of it will probably end up being at least partially manual. It'll be a few people on a team, imagining what could go wrong, figuring out what it looks like in a log if it does, and then deciding what happens if what you're looking for is found. 
- Access - Give the builders and end-users enough access that they can do what they need, and not more. Make sure that you have a process in place for establishing who gets access to what. 
- Environment segmentation - You should be able to deploy things to production without doing it manually, and you'll want to have copies of things in different geographic regions of the world to help lessen the risk of your hard work vanishing into thin air. This means you'll need to have some DevOps expertise to get things up and going in the right places. You'll have to either bring it to the project yourself, or you'll have to get someone with these skills on the team. 

3 - As much as you're able, pick simpler tools. It's remarkable how quickly things get complex, so pick tools and make decisions about your system that are simple whenever possible. A big system build will usually require individuals to specialize in the details of some part. Most of the time, those individuals will develop some expertise around that area and will optimize that part of the system in a way that increases complexity. So there's a natural tendency towards complexity with a new system build, mostly because they are many ways to increase complexity and relatively fewer ways to make things more straightforward. Optimizing one part of a system, though, often leads to decreased performance of the overall system. Picking simple tools can help balance out some of the inherent complexity. 

Here's a simple example: I prefer Python for data wrangling work. Compared to SQL, Python is probably a better tool! So if I'm doing work where I'm trying to reshape data where it isn't interacting with a system, I would use Python and not SQL. But managing a dynamic system with a Python component adds a layer of complexity: managing environments, upgrades, dependencies, and security, just to name a few. This additional complexity might help with the data wrangling, but it probably degrades the overall goal of the system: being able to get data out of our application and reshaped into a form that can be used by the business to improve performance. 

That's it for now. Next, I'll document the tools that we used and why they made sense for us. 
